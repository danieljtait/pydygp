{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nFitting of the MLFM-MixSA Model\n===============================\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pydygp.linlatentforcemodels import MLFMMixSA\nfrom sklearn.gaussian_process.kernels import RBF\nfrom pydygp.liealgebras import so\nfrom collections import namedtuple\nnp.set_printoptions(precision=3, suppress=True)\n\nmlfm = MLFMMixSA(so(3), R=1, order=10, lf_kernels=[RBF(), ])\n\nx0 = np.eye(3)  # initial conditions the std. basis vectors of R3\nN_outputs = 3\n\nbeta = np.random.normal(size=6).reshape(2, 3)\nbeta /= np.linalg.norm(beta, axis=1)[:, None]\n\ntt = np.linspace(0., 6., 10)\nData, gtrue = mlfm.sim(x0, tt, beta=beta, size=N_outputs)\n\nmlfm._setup_times([tt]*N_outputs, h=.25)\n\n# set the fixed points\n_ifix = np.linspace(0, tt.size-1, 5, dtype=np.intp)[1:-1]\nifix = [mlfm.data_inds[0][i] for i in _ifix]\n\nimport autograd.numpy as anp\nNormal = namedtuple('Normal', 'logpdf')\nbetaprior = Normal(lambda x: -0.5 * anp.sum(x**2) - \\\n                   0.5 * x.size * np.log(2 * np.pi * 1.))\n\n\n\ninit_opts = {'g0': gtrue[0](mlfm.ttc),\n             'g_is_fixed': False,\n             'beta_is_fixed': True,\n             'beta0': beta,\n             'mu_ivp_is_fixed': True,\n             'beta_prior': betaprior\n             }\n\noptim_opts = {\n    'options': {'disp': True,\n                'maxiter': 20}\n    }\n\nmlfm.fit([(tt, Y) for Y in Data],\n         ifix,\n         max_nt=10,\n         optim_opts=optim_opts,\n         verbose=True,\n         **init_opts)\n\nmu_ivp = np.array([\n    np.dstack([Y[_ifx, :] for Y in Data])\n    for _ifx in _ifix])\nmu_ivp = mu_ivp[:, 0, ...]\n\n\npi = np.ones(len(ifix)) / len(ifix)\nr = mlfm._get_responsibilities(pi, gtrue[0](mlfm.ttc), beta, mu_ivp, 1000)\n\nfig, ax = plt.subplots()\nax.plot(tt, r[0])\n\nfig2, ax2 = plt.subplots()\nax2.plot(tt, mlfm.g_[mlfm.data_inds[0]], '+')\nax2.plot(mlfm.ttc, gtrue[0](mlfm.ttc), 'k-', alpha=0.5)\n\nlayer = mlfm._forward(\n    gtrue[0](mlfm.ttc),\n    #mlfm.g_,\n    beta,\n    mlfm.mu_ivp_[0],\n    mlfm._ifix[0])\nprint(layer.shape)\nfig3, ax3 = plt.subplots()\n#ax3.plot(mlfm.ttc, layer[..., 0], 'k-', alpha=0.3)\nax3.plot(mlfm.ttc, layer[:, 0, 0], 'b-.')\n#ax3.plot(tt, layer[mlfm.data_inds[0], :, 0], 'b-.')\nax3.plot(tt, Data[0][:, 0], 's')\nax3.set_ylim((-1.1, 1.1))\n\nfig4, ax4 = plt.subplots()\nAr = [sum(brd*Ld for brd, Ld in zip(br, mlfm.basis_mats))\n      for br in beta]\nAr_ = [sum(brd*Ld for brd, Ld in zip(br, mlfm.basis_mats))\n       for br in mlfm.beta_]\n\nax4.plot(mlfm.ttc, Ar[0][0, 1] + Ar[1][0, 1]*gtrue[0](mlfm.ttc), 'k-')\nax4.plot(mlfm.ttc, Ar_[0][0, 1] + Ar_[1][0, 1]*mlfm.g_, 'C0-')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}