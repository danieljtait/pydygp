{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nVariational Inference\n=====================\n\nThis example presents an illustration of using the MLFM to\nlearn the model\n\n\\begin{align}\\dot{\\mathbf{x}}(t) = \\mathbf{A}(t)\\mathbf{x}(t)\\end{align}\n\nwhere $A(t) \\in \\mathfrak{so}(3)$ and $\\| x_0 \\| = 1$.\n\nThis note will also demonstrate the process of holding certain variables\nfixed as well as defining priors\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom pydygp.probabilitydistributions import (GeneralisedInverseGaussian,\n                                             InverseGamma,\n                                             Normal)\nfrom sklearn.gaussian_process.kernels import RBF\nfrom pydygp.liealgebras import so\nfrom pydygp.linlatentforcemodels import (MLFMAdapGrad,\n                                         GibbsMLFMAdapGrad,\n                                         VarMLFMAdapGrad)\nnp.random.seed(15)\nnp.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our first step is to initialise the models and then simulate some data.\n\nMake the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vmlfm = VarMLFMAdapGrad(so(3), R=1, lf_kernels=[RBF(),])\ngmlfm = GibbsMLFMAdapGrad(so(3), R=1, lf_kernels=[RBF(),])\n\nbeta = np.row_stack(([0.]*3,\n                     np.random.normal(size=3)))\n\n# simulate some initial conditions\nx0 = np.random.normal(size=6).reshape(2, 3)\nx0 /= np.linalg.norm(x0, axis=1)[:, None]\n\n# Time points to solve the model at\ntt = np.linspace(0., 6, 7)\n\n# Data and true forces\nData, g0 = vmlfm.sim(x0, tt, beta=beta, size=2)\n# vectorised and stack the data\nY = np.column_stack((y.T.ravel() for y in Data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specifying priors\n-----------------\n.. currentmodule:: pydygp.probabilitydistributions\n\nTo work with the pydygp all we require is that the :py:obj:`Prior`\nobject should have a method :py:meth:`~Prior.loglikelihood`, which\nshould have two arguments, the value of the parameter and an optional\nboolean to return the gradient. For example the following would be\na valid way of defining your own prior using a simple class constructed\nfrom a :py:class:`collections.namedtuple` object\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n\n# simple class with a 'loglikelihood' attribute\nMyPrior = namedtuple('MyPrior', 'loglikelihood')\n\n# improper uniform prior\ndef unif_loglik(x, eval_gradient=False):\n    if eval_gradient:\n        x = np.atleast_1d(x)\n        return 0, np.zeros(x.size)\n    else:\n        return 0.\n\nuniform_prior = MyPrior(unif_loglik)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ".... [clean up]\nspecifying if the gradient \n which returns the value of prior loglikelihood\nat :code:`x` and optionally it's gradient.\nTo work correctly the specified prior should also respect the\ntransformations described in the `Table <mlfm-ag-tutorials-partab>`.\n\nSome pre-existing priors are contained in\n:py:mod:`pydygp.probabilitydistributions`, and also include simple\noptions to get the prior for simple transformations of the random\nvariables including scale transforms and log transforms.\n\nHere we take the prior of the latent forces, which for the RBF kernel\ncorrespond to the length scale parameter of the kernel to have a\ngeneralised inverse Gaussian distribution. But because we are working\nwith the log transform of the length scale we use the\n:py:meth:`pydygp.probabilitydistributions.GeneralisedInverseGaussian.logtransform`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logpsi_prior = GeneralisedInverseGaussian(a=5, b=5, p=-1).logtransform()\nloggamma_prior = InverseGamma(a=0.001, b=0.001).logtransform()*vmlfm.dim.K\n\nbeta_prior = Normal(scale=1.) * beta.size\n\nfitopts = {'logpsi_is_fixed': True, 'logpsi_prior': logpsi_prior,\n           'loggamma_is_fixed': True, 'loggamma_prior': loggamma_prior,\n           'beta_is_fixed': True, 'beta_prior': beta_prior,\n           'beta0': beta,\n           }\n\n# Fit the model\nres, Eg, Covg, _, _ = vmlfm.varfit(tt, Y, **fitopts)\n\n\nGrv = gmlfm.gibbsfit(tt, Y, **fitopts, mapres=res)\n\nmg, cg = gmlfm.g_condpdf_mo(Y, beta,\n                            logphi=res.logphi,\n                            logpsi=res.logpsi,\n                            gamma=np.exp(res.loggamma))\n\nLapcov = res.optimres.hess_inv[:vmlfm.dim.N*vmlfm.dim.R,\n                               :vmlfm.dim.N*vmlfm.dim.R]\n\nfig, ax = plt.subplots()\n#ax.plot(tt, res.g.T, '+')\nax.plot(tt, Grv['g'].T, 'k+', alpha=0.2)\n#ax.plot(tt, Eg, 'o')\n#ax.errorbar(tt, res.g.T, yerr = 2*np.sqrt(np.diag(Lapcov)), fmt='s')\n#ax.errorbar(tt, Eg, yerr = 2*np.sqrt(np.diag(Covg[..., 0, 0])), fmt='o')\n\nttdense = np.linspace(0., tt[-1])\nax.plot(ttdense, g0[0](ttdense), 'k-', alpha=0.2)\nfpred, fstd = vmlfm.predict_lf(ttdense, return_std=True)\nvfpred, fstd2 = vmlfm.var_predict_lf(ttdense, True)\nvfpred, fstd3 = vmlfm.var_predict_lf(ttdense, True)\nax.plot(ttdense, vfpred[0, :], 'r--')\n\nax.fill_between(ttdense,\n                fpred[0, :] + 2*fstd[0, :],\n                fpred[0, :] - 2*fstd[0, :],\n                alpha=0.3)\n\n#gp = vmlfm.latentforces[0]\n#M = gp.kernel_(ttdense[:, None], vmlfm.ttc[:, None])\n#M = M.dot(np.linalg.inv(gp.kernel_(vmlfm.ttc[:, None])))\n#C = M.dot(Covg[..., 0, 0].dot(M.T))\n#sd = np.sqrt(np.diag(C))\n\nax.fill_between(ttdense,\n                vfpred[0, :] + 2*fstd3[0, :],\n                vfpred[0, :] - 2*fstd3[0, :],\n                facecolor='red',\n                alpha=0.3)\n\nprint(cg.shape)\nfig, ax = plt.subplots()\n\nax.errorbar(tt, mg, yerr=2*np.sqrt(np.diag(cg)), xerr=None, capsize=20)\n\nax.plot(ttdense, g0[0](ttdense), 'k-', alpha=0.2)\nax.plot(ttdense, vfpred[0, :], 'r-')\n\nax.fill_between(ttdense,\n                vfpred[0, :] + 2*fstd3[0, :],\n                vfpred[0, :] - 2*fstd3[0, :],\n                facecolor='red',\n                alpha=0.2)\n\nplt.show()\n\n\n\n\"\"\"\nttdense = np.linspace(tt[0], tt[-1], 50)\nCff_ = vmlfm.latentforces[0].kernel_(ttdense[:, None], tt[:, None])\nCf_f_ = vmlfm.latentforces[0].kernel_(tt[:, None])\nCf_f_[np.diag_indices_from(Cf_f_)] += 1e-5\nLf_f_ = np.linalg.cholesky(Cf_f_)\n\nfrom scipy.linalg import cho_solve\ngpred = Cff_.dot(cho_solve((Lf_f_, True), Eg))\nprint(np.sqrt(np.diag(Covg[..., 0, 0])))\nax.plot(ttdense, gpred, 'r-.')\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}