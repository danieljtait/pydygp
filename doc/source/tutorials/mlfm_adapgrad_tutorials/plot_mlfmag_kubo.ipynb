{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\nKubo Oscillator\n===============\n\nThis note continues on from the `basic MAP tutorial<tutorials-mlfm-ag>`\nexamining the Adaptive Gradient matching approximation the MLFM.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom pydygp.liealgebras import so\nfrom pydygp.linlatentforcemodels import MLFMAdapGrad\nfrom pydygp.probabilitydistributions import Normal, Laplace\nfrom sklearn.gaussian_process.kernels import RBF\n\nnp.random.seed(12345)\n\nmlfm = MLFMAdapGrad(so(2), R=1, lf_kernels=(RBF(), ))\n\n\nx0 = np.array([1., 0.])\nbeta = np.array([[0., ], [1., ]])\n\nttd = np.linspace(0., 5., 100)\ndata, lf = mlfm.sim(x0, ttd, beta=beta)\n\ntt = ttd[::10]\nY = data[::10, :]\n\nmapres = mlfm.fit(tt, Y.T.ravel(),\n                  logpsi_is_fixed=True,\n                  beta_is_fixed=True, beta0=beta)\ngpred = mlfm.predict_lf(ttd)\n\nfig, ax = plt.subplots()\nax.plot(ttd, lf[0](ttd), 'k-', alpha=0.3)\nax.plot(ttd, gpred[0], 'C0-')\nprint(mapres.optimres.fun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$\\beta$ free\n==================\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mapres2 = mlfm.fit(tt, Y.T.ravel(),\n                   beta0=beta, logpsi_is_fixed=True)\ngpred2 = mlfm.predict_lf(ttd)\nax.plot(ttd, gpred2[0], 'r-')\nprint(mapres2.optimres.fun)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So whats happened? The latent force looks like it has\ncollapsed to a constant valued function. Lets plot just\nfunction itself to get an idea what's going on\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig2, ax2 = plt.subplots()\nax2.plot(ttd, gpred2[0], 'r-')\n\nfrom scipy.interpolate import interp1d\nginterp = interp1d(ttd, gpred2[0],\n                   kind='cubic', fill_value='extrapolate')\n\nfig3, ax3 = plt.subplots()\ndata2, _ = mlfm.sim(x0, ttd,\n                    beta=mapres2.beta, latent_forces=(ginterp, ))\nax3.plot(ttd, data2, 'C0-')\nax3.plot(tt, Y, 'o')\n\nbeta_prior = Normal() * Normal()\n\nmapres3 = mlfm.fit(tt, Y.T.ravel(),\n                   beta0=beta,\n                   logpsi_is_fixed=True,\n                   beta_prior = beta_prior)\nprint(mapres3.optimres.fun)\nprint(mapres2.beta)\nprint(mapres3.beta)\ngpred3 = mlfm.predict_lf(ttd)\n\nfig4, ax4 = plt.subplots()\nax4.plot(ttd,\n         mapres3.beta[0, 0] + mapres3.beta[1, 0]*gpred3[0],\n         'C0--')\nax4.plot(ttd, lf[0](ttd), 'k-', alpha=0.3)\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}