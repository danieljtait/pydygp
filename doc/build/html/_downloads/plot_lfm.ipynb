{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nFitting the LFM Using the MLFM\n==============================\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.gaussian_process.kernels import RBF\nfrom scipy.integrate import odeint\nfrom pydygp.kernels import LFMOrder2Kernel, WhiteKernel\nfrom pydygp.gaussian_process import MultioutGaussianProcessRegressor\nfrom pydygp.linlatentforcemodels import MLFMAdapGrad\nfrom pydygp.liealgebras import so\n\nC = [0.5,]\nD = [1.0,]\nS = [1.0,]\n\nA = np.array([[0., 1.],\n              [-D[0], -C[0]]])\n\ndef f(t):\n    z = (t - 2.5)/0.5\n    return np.exp(-.5*z**2)*(1-z**2)\n\ndef dXdt(X, t):\n    v = A.dot(X)\n    v[1] += S[0]*f(t)\n    return v\n\nttdense = np.linspace(0., 10., 100)\nsol = odeint(dXdt, [0., 0.], ttdense)\n\ntt = ttdense[1::7]\nY = sol[1::7, 0]\nvecY = Y.T.ravel()  # vectorisation of Y\n\nkernel = LFMOrder2Kernel(C=C, D=D, S=S)\nlfm = MultioutGaussianProcessRegressor(kernel)\n\n_rv = lfm.sample_y(tt[:, None], n_samples=1, random_state=2)\n\nlfm.fit(tt[:, None], vecY[:, None])\nprint(lfm.kernel_.theta)\n\nL1 = np.array([[ 0., 1., 0.],\n               [ 0., 0., 0.],\n               [ 0., 0., 0.]])\nL2 = np.array([[ 0., 0., 0.],\n               [-1., 0., 0.],\n               [ 0., 0., 0.]])\nL3 = np.array([[ 0., 0., 0.],\n               [ 0.,-1., 0.],\n               [ 0., 0., 0.]])\nL4 = np.array([[ 0., 0., 0.],\n               [ 0., 0., 1.],\n               [ 0., 0., 0.]])\nbeta = np.array([[1., 1., 0.5, 0.],\n                 [0., 0., 0., 1.]])\n\n_Y = sol[1::7]\n\ns1 = np.random.normal(size=2)\ns2 = np.random.normal(size=2)\n\nY1 = _Y + s1[None, :]\nY2 = _Y + s2[None, :]\n\n_Z1 = np.column_stack((Y1, np.ones(tt.size)))\n_Z2 = np.column_stack((Y2, np.ones(tt.size)))\n\nZ = np.column_stack((sol[1::7], np.ones(tt.size)))\nz = np.column_stack((_Z1.T.ravel(), _Z2.T.ravel()))\n\n\nmlfm = MLFMAdapGrad((L1, L2, L3, L4), R=1, lf_kernels=[RBF(), ])\nbeta_fix_inds = np.array([[True, False, False, True],\n                          [True, True, True, True]])\n\nfrom pydygp.probabilitydistributions import Normal\nbeta_prior = Normal() * 3\nres_ag = mlfm.fit(tt, Z.T.ravel()[:, None],\n                  logpsi_is_fixed=True,\n                  logphi_is_fixed=True,\n                  beta_is_fixed=True, beta0=beta)\n\nprint(res_ag.beta)\nEg, SDg = mlfm.predict_lf(ttdense, return_std=True)\n\n\nA = [sum(brd*Ld for brd, Ld in zip(br, (L1, L2, L3, L4)))\n     for br in res_ag.beta]\n\nfrom scipy.interpolate import interp1d\nu = interp1d(ttdense, Eg[0, :], kind='cubic', fill_value='extrapolate')\n\nsol2 = odeint(lambda x, t: (A[0] + A[1]*u(t)).dot(x),\n              [0., 0., 1.], ttdense)\n\n\n#fig, ax = plt.subplots()\n#ax.plot(tt, Y, 'o')\n#ax.plot(ttdense, sol2[:, :-1], '-', alpha=0.3)\n\nfig2, ax2 = plt.subplots()\nax2.plot(ttdense, f(ttdense), 'k-', alpha=0.3)\nax2.plot(tt, res_ag.g.T, '+')\nax2.plot(ttdense, Eg.T, '--')\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}