{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nVariational Inference\n=====================\n\nThis example presents an illustration of using the MLFM to\nlearn the model\n\n\\begin{align}\\dot{\\mathbf{x}}(t) = \\mathbf{A}(t)\\mathbf{x}(t)\\end{align}\n\nwhere $A(t) \\in \\mathfrak{so}(3)$ and $\\| x_0 \\| = 1$.\n\nThis note will also demonstrate the process of holding certain variables\nfixed as well as defining priors\n\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nimport numpy as np\nfrom pydygp.probabilitydistributions import (GeneralisedInverseGaussian,\n                                             InverseGamma)\nfrom sklearn.gaussian_process.kernels import RBF\nfrom pydygp.liealgebras import so\nfrom pydygp.linlatentforcemodels import (MLFMAdapGrad,\n                                         GibbsMLFMAdapGrad,\n                                         VarMLFMAdapGrad)\nnp.random.seed(15)\nnp.set_printoptions(precision=3, suppress=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Our first step is to initialise the models and then simulate some data.\n\nMake the model\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "vmlfm = VarMLFMAdapGrad(so(3), R=1, lf_kernels=[RBF(),])\ngmlfm = GibbsMLFMAdapGrad(so(3), R=1, lf_kernels=[RBF(),])\n\nbeta = np.row_stack(([0.]*3,\n                     np.random.normal(size=3)))\n\n# simulate some initial conditions\nx0 = np.random.normal(size=6).reshape(2, 3)\nx0 /= np.linalg.norm(x0, axis=1)[:, None]\n\n# Time points to solve the model at\ntt = np.linspace(0., 6, 7)\n\n# Data and true forces\nData, g0 = vmlfm.sim(x0, tt, beta=beta, size=2)\n# vectorised and stack the data\nY = np.column_stack((y.T.ravel() for y in Data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Specifying priors\n-----------------\n.. currentmodule:: pydygp.probabilitydistributions\n\nThe prior should have a loglikelihood(x, eval_gradient=False) method\nwhich returns the loglikelihood of the prior variable at x and\nand optionally its gradient.\n\nPreexisting priors are contained in :py:mod:`pydygp.probabilitydistributions`\n\nfor example there is the class `ProbabilityDistribution`\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "logpsi_prior = GeneralisedInverseGaussian(a=5, b=5, p=-1).logtransform()\nloggamma_prior = [InverseGamma(a=0.001, b=0.001).logtransform(),]*vmlfm.dim.K\n\n# Fit the model\nres, Eg, Covg = vmlfm.varfit(tt, Y,\n                             logtau_is_fixed=False,\n                             logpsi_is_fixed=False, logpsi_prior=logpsi_prior,\n                             loggamma_is_fixed=False, loggamma_prior=loggamma_prior,\n                             beta_is_fixed=True, beta0=beta)\nprint(res.logtau)\nGrv = gmlfm.gibbsfit(tt, Y, mapres=res)\n\nLapcov = res.optimres.hess_inv[:vmlfm.dim.N*vmlfm.dim.R,\n                               :vmlfm.dim.N*vmlfm.dim.R]\n\nfig, ax = plt.subplots()\n#ax.plot(tt, res.g.T, '+')\nax.plot(tt, Grv['g'].T, 'k+', alpha=0.2)\n#ax.plot(tt, Eg, 'o')\n#ax.errorbar(tt, res.g.T, yerr = 2*np.sqrt(np.diag(Lapcov)), fmt='s')\n#ax.errorbar(tt, Eg, yerr = 2*np.sqrt(np.diag(Covg[..., 0, 0])), fmt='o')\n\nttdense = np.linspace(0., tt[-1])\nax.plot(ttdense, g0[0](ttdense), 'k-', alpha=0.2)\nfpred, fstd = vmlfm.predict_lf(ttdense, return_std=True)\n\nax.plot(ttdense, fpred[0, :], '-.')\nax.fill_between(ttdense,\n                fpred[0, :] + 2*fstd[0, :],\n                fpred[0, :] - 2*fstd[0, :],\n                alpha=0.3)\n\nplt.show()\n\n\n\n\"\"\"\nttdense = np.linspace(tt[0], tt[-1], 50)\nCff_ = vmlfm.latentforces[0].kernel_(ttdense[:, None], tt[:, None])\nCf_f_ = vmlfm.latentforces[0].kernel_(tt[:, None])\nCf_f_[np.diag_indices_from(Cf_f_)] += 1e-5\nLf_f_ = np.linalg.cholesky(Cf_f_)\n\nfrom scipy.linalg import cho_solve\ngpred = Cff_.dot(cho_solve((Lf_f_, True), Eg))\nprint(np.sqrt(np.diag(Covg[..., 0, 0])))\nax.plot(ttdense, gpred, 'r-.')\n\"\"\""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}