import numpy as np
from scipy.stats import multivariate_normal
from pydygp.kernels import Kernel
from scipy.optimize import minimize

class GaussianProcess:
    def __init__(self, kernel):
        assert(isinstance(kernel, Kernel))
        self.kernel = kernel

        self.is_mean_zero = True


    def fit(self, x, y=None, kpar=None):

        if kpar is None:
            kpar = self.kernel.kpar
        else:
            self.kernel.kpar = kpar
        
        self.inputs = x
        self.outputs = y

        self.cov = self.kernel.cov(x)

    def sim(self, x=None):
        if x is None:
            x = self.inputs
        cov = self.kernel.cov(x)
        try:
            cov_chol = np.linalg.cholesky(cov)
        except:
            cov += np.diag(1e-5*np.ones(cov.shape[0]))
            cov_chol = np.linalg.cholesky(cov)
            
        if self.is_mean_zero:
            mean = np.zeros(cov.shape[0])
        return multivariate_normal.rvs(mean, cov)

    def pred(self, xnew, return_var=False):

        if isinstance(xnew, float):
            xnew = np.array([xnew]).reshape(1,1)
    

        C22 = self.cov
        C22inv = np.linalg.inv(C22)

        C12 = self.kernel.cov(xnew,
                              x2=self.inputs)
                              
        C11 = self.kernel.cov(xnew)

        mcond = np.dot(C12, np.dot(C22inv, self.outputs))
        if return_var:
            ccond = C11 - np.dot(C12, np.dot(C22inv, C12.T))
            return mcond, ccond
        else:
            return mcond


    def loglikelihood(self, y, x=None, kpar=None,
                      return_cov=False):

        if self.is_mean_zero:
            mean = np.zeros(self.inputs.size)

        if kpar is not None and x is None:
            cov = self.kernel.cov(self.inputs, kpar=kpar)

        elif kpar is not None and x is not None:
            cov = self.kernel.cov(x, kpar=kpar)

        else:
            cov = self.cov

        try:
            lp = multivariate_normal.logpdf(y, mean=mean, cov=cov)
        except:
            lp = -np.inf

        # Optionally return the calculated covariance
        # for more efficient updating
        if return_cov:
            return lp, cov
        else:
            return lp


    def hyperpar_optim(self, y):

        def _objfunc(kpar):
            try:
                return -self.loglikelihood(y, kpar=kpar)
            except:
                return np.inf

        def _objfunc_grad(kpar):
            dCdp = self.kernel.cov_par_grad(kpar, self.inputs)

            if self.is_mean_zero:
                m = np.zeros(y.size)
                
            cov = self.kernel.cov(self.inputs, kpar=kpar)
            dOdC = -_mvt_loglik_grad(y, m, cov)

            return [np.sum(dOdC*item) for item in dCdp]

        cons = ({'type': 'ineq', 'fun': lambda x:  x},)

        res = minimize(_objfunc, x0=self.kernel.kpar,
                       jac=_objfunc_grad,
                       method='SLSQP',
                       constraints=cons)

        return res.x, res.success
        
    def hyperpar_mh_update(self, y, kpar_cur,
                           kpar_proposal,
                           kpar_prior,
                           lpcur=None,
                           store_new_par=False,
                           **kwargs):

        if lpcur is None:
            lpcur = self.loglikelihood(y, kpar=kpar_cur)

        kpar_new, prop_ratio = kpar_proposal.rvs(kpar_cur, **kwargs)

        pnew = kpar_prior.pdf(kpar_new)
        if pnew > 0:

            lpnew, cnew = self.loglikelihood(y, kpar=kpar_new, return_cov=True)

            lpcur += kpar_prior.logpdf(kpar_cur)
            lpnew += kpar_prior.logpdf(kpar_new)

            A = np.exp(lpnew - lpcur)*prop_ratio
            if np.random.uniform() <= A:
                if store_new_par:
                    self.kernel.kpar = kpar_new
                    self.cov = cnew
                return kpar_new, lpnew, True

        return kpar_cur, lpcur, False


def _mvt_loglik_grad(y, m, cov):
    cov_inv = np.linalg.inv(cov)
    eta = y - m
    etaetaT = np.outer(eta, eta)
    expr = np.dot(cov_inv, np.dot(etaetaT, cov_inv))

    return -0.5*(cov_inv - expr)
